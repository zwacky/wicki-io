[{"content":"  Learn why WOFF2, and WOFF as a fallback, is the only font format you need.\nIf you’ve been around as a web developer for some time, you’ve seen these font formats in your CSS. Here’s an example for a self-hosted Raleway font:\n@font-face { font-family: 'Raleway'; font-style: normal; font-weight: 400; src: url('../fonts/raleway-v22-latin-regular.eot'); /* IE9 Compat Modes */ src: local(''), url('../fonts/raleway-v22-latin-regular.eot?#iefix') format('embedded-opentype'), /* IE6-IE8 */ url('../fonts/raleway-v22-latin-regular.woff2') format('woff2'), /* Super Modern Browsers */ url('../fonts/raleway-v22-latin-regular.woff') format('woff'), /* Modern Browsers */ url('../fonts/raleway-v22-latin-regular.ttf') format('truetype'), /* Safari, Android, iOS */ url('../fonts/raleway-v22-latin-regular.svg#Raleway') format('svg'); /* Legacy iOS */ } Do we still need to support all 5 (!) of them? Where did they come from? How do they compare to each other?\nThese questions I’d like to answer in this post.\n Timeline   Historical releases of nowadays still used font format\n  It took time and iterations to have what we’ve ended up with.\nWith more efficient font formats, thanks to compression, browsers were able to support them.\nHave a look at the elaborate Font Format Timeline by Pedro Amado. This shows, that there are in fact many more facets to font formats than we think.\n Comparison I’ll go ahead and use the following criteria for my score to compare font formats:\n Browser support as of November 2021 File size (aka format efficiency)  For the file size I used Comic Sans Neue and the browser support I gathered from caniuse.com. Here’s the let’s have a look at the following table ordered historically with these criteria:\n   Font Format Release year Browser support File size Score     OTF/TTF 1985 98.6% 57 kb 69/100   SVG 2001 19.4% 113 kb 20/100   EOT 2007 0.87% 27 kb 43/100   WOFF 2009 98.6% 30 kb 88/100   WOFF2 2017 96.8% 23 kb 98/100    Let me explain why WOFF2 scored higher than WOFF in this comparison: It all comes down to the 7 kb in file size. Generally if your site shows the user content more quickly, your conversion will increase—even if the gain is only in the hundred millisecond range.\nYour fonts could be:\n blocking content from appearing (invisible text) cause layout shifts delay further assets from downloading on flaky mobile networks sending bad signals to Google (LCP \u0026 CLS from Core Web Vitals)   How to convert It’s a very good idea to self host your fonts for performance reasons. I described one case in my Time to Say Goodbye to Google Fonts post.\nHow to best convert a font type to WOFF2 and WOFF depends where you got your font type from:\n Google Fonts: Download the used font types on Google Fonts and use google-webfonts-helper to download the other font formats. everything else: Use Font Squirrels’s Webfont Generator. The ‘optimal’ setting even only spits out WOFF2 and WOFF files.   Conclusion WOFF2, and WOFF as fallback, is enough nowadays¹.\n¹) if your site doesn’t need support for antiquated browsers like IE8.\n If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter.\nWOFF2, and WOFF* as a fallback, is the only font format you need in 2021.\n*) if you care about IE9-IE11. Thanks 𝙾̶𝚋̶𝚊̶𝚖̶𝚊̶ IE.\nRead on! I'm diving in this thread into the TIMELINE of font formats and how they COMPARE with each other. 🚀 [1/6] pic.twitter.com/nkcDTcmMoc\n— Simon Wicki (@zwacky) November 25, 2021  ","description":"Learn why WOFF2, and WOFF as a fallback, is the only font format you need.","image":"woff2.png","keywords":["woff2,","woff,","font","format"],"link":"/posts/2021-11-woff2-one-font-format-to-rule-them-all/","title":"One Font Format to Rule Them All: WOFF2"},{"content":"Are you a developer who is concerned about the size of newly added libraries? Or do you want to find a culprit in a rather big Javascript bundle?\nIf you’re like me, then you answered yes to both questions.\nIn this post I’ll cover a few tools that come in handy for a quick analysis of bundle sizes without changing or ejecting your build architecture.\n VS Code extension: Import Cost   Immediately see the weight of what you import in VS Code.\n  Understand the cost of an import early.\nThis extension will display inline in the editor the size of the imported package. It supports tree shaking, so the size should be displayed correctly for a few exported functions.\nWith this you may spot mistakes like these:\nimport moment from 'moment'; // 289.7KB moment.now(); import { now } from 'moment'; // 0.082KB now(); It’s also available for JetBrains IDE, Atom and Vim.\n👉 https://github.com/wix/import-cost\n Website: Bundlephobia   This website lets you search for libraries and display their sizes without the need to install. It shows the size of each version and even suggests alternatives to similar libraries that might be lighter—talking about a new framework or library every week.\nYou could also drop your package.json file and order it by size to see your biggest libraries. Personally I find this quite fun, but usually I use this tool to check bundle sizes of not-yet-installed libraries.\n👉 https://bundlephobia.com/\n NPM: source-map-explorer   Like the name suggests you need to build source maps. With modern framework CLIs it’s enabled by default in prod builds.\n Useful tool for imported package visualisation in relation to their size. By clicking on the packages, you can further inspect their sizes and children.\n👉 npx source-map-explorer ./dist *.js 👉 https://github.com/danvk/source-map-explorer\n Website: PageSpeed Insight / Lighthouse If your site is already public you can use Google’s PageSpeed Insight to detect big Javascript bundles.\nBonus: It also includes Javascript files, that are downloaded on runtime from your ad networks, Google Tag Manager and other tools.\n👉 https://developers.google.com/speed/pagespeed/insights/\nCheck out this tweet to see the treemap in action.\n If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. Are you afraid of the Unknown? 😨\nWorry no more! 😌🧘‍♀️\nHere's a this list of 4 useful tools to keep your Javascript bundle size in check.\nIt's comforting to know what size you import and what lib uses the most. 1/6 pic.twitter.com/g61BUCST3y\n— Simon Wicki (@zwacky) August 18, 2021  ","description":"Check out these tools to have an overview of your Javascript bundle size.","image":"bundle-weight.png","keywords":["webperf,","javascript,","webpack,","bundle","size"],"link":"/posts/2021-08-keep-your-bundle-size-in-check/","title":"Keep Your Javascript Bundle Size in Check"},{"content":"Have you ever optimised your website for the Core Web Vitals (CWV)? Did you want to check your changes the next day—but Google’s various tools don’t give you current daily CWV metrics due to the rolling 28-day window?\nIn this post I’ll review Google’s toolset for measuring CWV and explain how to see if your changes had any effect on CWV day by day with Google Analytics (GA).\n Accurate daily CWV measurements: Create your report   3-step workflow to get your daily accurate CWV report.\n  There isn’t a prebuilt view for daily accurate reports in any analytics service out of the box. That’s why we need to create our own report in three steps:\n web-vitals: to measure CWV metrics in the frontend CWV GA tracking snippet: to send these captured metrics over to GA Web Vitals Report: to create your CWV report by connecting your GA with this external reporting tool  The web-vitals library lets us grab all CWV metrics of each visitor of the site. These metrics are then sent off to Google Analytics. The external Web Vitals Report tool can then extract these tracking events from your GA account and create a daily accurate report.\nDon’t be fooled: GA can display your CLS, LCP and other CWV metrics when you search for these events. But they’re inaccurate in the GA events view. For instance it’s possible that multiple CLS events are sent, each with their own delta value that accumulates to the total CLS. You’d need to group by that day and url and user to get an accurate reporting. This grouping isn’t supported out of the box.\nThat’s why Google created the external tool Web Vitals Report. You can connect your GA account with it and it will extract the data. If you followed the default naming inside of the CWV GA tracking snippet, everything will work out of the box. You’ll be rewarded with shiny graphs and an accurate* report overall.\n*) GA imposes a limit in each report which can lead to sampling and an inaccurate report.   Core Web Vitals reports are a great way to group metric by day and overall break downs.\n  The screenshot above is an excerpt of the Web Vitals Report with daily accurate Core Web Vital metrics.\n Inaccurate daily CWV measurements: Google’s tools For daily CWV measurements we can’t rely on Google’s tools because they either aggregate over a long 28-day period or only measure single pages with synthetic, non Real-User Metrics (RUM). So it takes a lot of days to see any change within that period.\nStill, Google provides a great—and very easy—overview of your CWV metrics to plan where to optimise next. But you can’t check the impact on CWV of your optimisation with these tools, e.g. the next day.\nField data aka real-user experiences:\n Google Search Console: measures similar pages PageSpeed Insights: measures a single page \u0026 all pages as a whole CrUX (Chrome UX Report): measures single pages \u0026 all pages as a whole  Lab data aka synthetic user experience:\n Lighthouse: measures a single page Chrome DevTools: measures a single page Web Vitals Chrome Extension: measures a single page   Rolling 28-day window explained I dedicate a paragraph to this in particular because the rolling 28-day window appears many times in CWV measurement—especially in how the field data is measured.\nImagine you had 27 days of awful Cumulative Layout Shift (CLS) of 1.0 and on day 28 you magically fix it to 0.\nAt day 28 your average CLS would be 0.96—which would be rounded up to 1.0 again:\nDays | 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ------------------------------------------------------------------------------------------------------------------------------------------------------- Daily CLS | 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 Average | 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.96 In this example above your 1.0 to 0 CLS optimisation would take +23 days until you reach the allowed mobile CLS of 0.1.\nSee in the following table how slowly the average is decreasing:\nDays | +1 +2 +3 +4 +5 +6 +7 +8 +9 +10 +11 +12 +13 +14 +15 +16 +17 +18 +19 +20 +21 +22 +23 +24 +25 +26 +27 +28 ------------------------------------------------------------------------------------------------------------------------------------------------------- Daily CLS | 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Average | 0.9 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.6 0.6 0.6 0.5 0.5 0.5 0.4 0.4 0.4 0.3 0.3 0.3 0.2 0.2 0.1 0.1 0.1 0.0 0.0 0.0 More resources  An In-Depth Guide To Measuring Core Web Vitals Google Forum: Explanation different data points in different tools Tracking Changes in Search Console’s Ore Web Vitals Report   If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. How did we turn 16k to 651k green URLs for more visibility by Google with #CoreWebVitals? ✅\nWith this cycle of getting a quicker feedback loop:\n1️⃣ Track \u0026 evaluate current daily measurements\n2️⃣ Identify what metrics and pages to work on\n3️⃣ Deploy\nRead on for the what \u0026 how. pic.twitter.com/0hVZ9UnkND\n— Simon Wicki (@zwacky) August 11, 2021  ","description":"Create accurate Core Web Vitals reports of current daily measurements, not only averaged over a rolling 28-day window.","image":"angular-ruler.png","keywords":["webperf,","Core","Web","Vitals,","SEO,","Google","Analytics"],"link":"/posts/2021-08-accurate-core-web-vitals-measurements/","title":"Accurate Daily Measurements of Core Web Vitals with Google Analytics"},{"content":"Do you…\n care about Core Web Vitals, especially Cumulative Layout Shift (CLS) use list rendering with a JS Framework (v-for, *ngFor, …)  If you answered both with yes, then please read on.\n  Slow connection device causes 0.51 CLS upon updating a list.\n  JS frameworks—such as Vue, Angular or React—can cache DOM nodes of list items*. Filtering a list will therefore be faster, because DOM nodes of list items can potentially be reused. This reuse is done by moving the DOM nodes around, instead of re-creating them.\nBut when a list changes and the nodes of list items merely switch their positions, the reused items are considered as a shift in the DOM (CLS) by Core Web Vitals on slow connection devices.\n*) Vue uses :key, React uses key, Angular uses *ngFor with trackBy\n Why only on slow connection devices? CLS takes user interaction into account. When content is changing after a click, it’s not counted towards the CLS until a grace period of 500ms. If the delay of content changing takes longer than 500ms, you’ll be penalised with the full CLS score.\nFast connection devices will usually finish updating the list before the grace period ends.\nKeep an eye out in your Google Search Console for URLs that are frequented by countries that are further away from your servers/CDNs and have a lower average mobile bandwidth.\n  A whopping 433k Indian URLs are affected here by list rendering on slow connection devices.\n   Solution Google’s calculation of CLS has been changed before, so I would love to see this here too. Until then it’s on us to not get penalised.\nWe’d have to make sure, that the DOM elements get re-created everytime a list gets updated or filtered. I wouldn’t advise to completely drop the unique key (:key). Rather think of a key for each list item that is tied to the request itself, e.g. :key=\"item.id + \"-\" + requestQuery\". The filtering itself will have a brief flash due to recreating the DOM node, but considering the Green URLs that Google will give you it’s worth it.\nWith this you can continue with infinite scroll and page navigation without perf loss.\n If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. How can Cumulative Layout Shift (CLS) happen with a simple list rendering? 🧮\nSee it in action in the gif:\nA whopping 0.51 CLS was produced with a mobile viewport and mid-tier connection.\nRead on to understand what it is and how you can make Google's Core Web Vitals happy. 👇 pic.twitter.com/nfC8nB7FQW\n— Simon Wicki (@zwacky) July 29, 2021  ","description":"This post explains a hidden caveat of list rendering in JavaScript frameworks in term of CLS.","image":"cls-slow-connection-devices.png","keywords":["Core","Web","Vitals,","SEO,","webperf,","Javascript"],"link":"/posts/2021-07-list-rendering-cls/","title":"How List Rendering Can Cause Huge Cumulative Layout Shift"},{"content":"  Passing your Core Web Vitals and google will reward you with visibility.\n  With Google’s June 2021 update Core Web Vitals (CWV) will become a factor in SEO ranking. It measures the quality of a site by these three metrics: LCP, FID and CLS.\nIf you pass all of them them, Google will reward you with more visibility.\nYou can check how well you do on these metrics via several ways:\n Pagespeed Insights Search Console WebDev Measure Chrome DevTools \u0026 Lighthouse Web Vitals Chrome Extension Custom event tracking with Google Analytics   Largest Contentful Paint (LCP) LCP marks the point in the page load timeline when the page’s main content has likely loaded.\n—web.dev\n   LCP will keep changing until the Largest Contentful Paint is done, which here is at 1.766s\n  The faster you make the biggest content on the above-the-fold appear, the better your metric will be.\nIf you can preload the biggest image already, e.g. with \u003clink rel=\"preload\" href=\"...\"\u003e or make the content appear without rendering it with your Javascript Framework, you’ll win big.\nCauses of LCP:\n Slow server response times Render-blocking Javascript and CSS Slow resource load times Client-side rendering   First Input Delay (FID) FID measures the time from when a user first interacts with a page to the time when the browser is actually able to begin processing event handlers in response to that interaction.\n—web.dev\n   0.666s pass until you see the website working with your interaction.\n  Javascript is synchronous and single-threaded. If a user interaction happens while the thread is busy, the user has to wait. If you have too many libraries that load at runtime, worse even if you don’t need them right away, this can increase the FID.\nIt’s similar to TTI (Time to Interactive) that you might have heard of already. However FID will only start to be measured from the user interaction. Whereas TTI will be measured from the very start of loading the site.\nCauses of FID:\n too much javascript code to execute right away, especially 3rd party ad networks can push lots of unneeded Javascript not using code-splitting (e.g. with webpack, Rollup, Parcel)   Cumulative Layout Shift (CLS) CLS is a measure of the largest burst of layout shift scores for every unexpected layout shift that occurs during the entire lifespan of a page.\n—web.dev\n   CLS is happening here that makes the content jump around.\n  Layout shifts only happen when an element higher up is making other elements move. If the element is changing and there is nothing after it, it doesn’t count as a layout shift—as well as DOM changes that were caused by a user interaction with a grace period (500ms).\nNote: Mobile viewports and 3G devices will cause much more CLS. Make sure to throttle your connection while optimising your page for these metrics.\nCauses of CLS:\n images without size attributes requests finish later that will inject content above existing content lazy loaded components   If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. CORE WEB VITALS 📈\nWith Google's June 2021 update CWV becomes a factor in SEO ranking. It measures the quality of a site by these three metrics: LCP, FID and CLS.\nIf you pass them, Google will reward you with more visibility.\nRead on what they are and how to measure them. 👇 pic.twitter.com/KSng9XaX1O\n— Simon Wicki (@zwacky) July 21, 2021  ","description":"The 3 new performance metrics that Google will rank against in its June 2021 update.","image":"core-web-vitals.png","keywords":["core","web","vitals,","LCP,","FID,","CLS,","google","udpate,","seo"],"link":"/posts/2021-07-core-web-vitals/","title":"Core Web Vitals explained with GIFs"},{"content":"  Keep the books you read in sync with your GitHub profile README\n  In this post I’d like to show you how to use GitHub Actions to automatically sync your Goodreads books you read in your GitHub profile README.\nI created goodreads-profile-workflow for devs that love to read and like to share what they read. You can customise the input parameters to your liking: list the books you’re currently reading, last 5 books you read and even add your personal ratings, too.\nLeave a star on the repo if you think it’s cool!\nGitHub Repo: https://github.com/zwacky/goodreads-profile-workflow\nGitHub Action Name: Goodreads Profile Workflow\n GitHub Actions and Profile READMEs   When you head over to your GitHub profile page, https://github.com/zwacky in my case, you’ll see that you can do more than just pin repositories. If you’re missing yours, check out the official docs on GitHub to get one, too.\nKeeping your profile README fresh with up-to-date data is a great way to show your visitors what you’re up to or what you’ve been working on. Either you do it manually or you automate it with the help of GitHub Actions. They can run workflows periodically and update your README for you.\nThere are tons of great ones out there. Have a look at the great list awesome-github-profile-readme repo for inspiration. They include GitHub Actions that can:\n update your latest blog entries update your GitHub activity or even update your sleep data from your sleep tracker   How To Use I’ll quickly explain here how to use goodreads-profile-workflow and what the GitHub Actions do. For a full step-by-step guide read the how to use section of the project.\n#1 Add placeholders to your README Your README.md will be updated and eventually updated every hour by the GitHub Action. So it needs to know where the list begins, and where it ends. The content inbetween will then be replaced with the latest content. As an example it could look like this:\n# Hey there 👋 I'm Simon and live in Berlin as a Freelance Frontend developer. # Last 5 Books I've Read 🤓 \u003c!-- GOODREADS-LIST:START --\u003e \u003c!-- GOODREADS-LIST:END --\u003e You can customize the GOODREADS-LIST tags, so you can have multiple workflows updating different sections in your README. This is also what I’m using to get to the screenshot at the top of this post.\n#2 Create a workflow file Create a file in your own GitHub profile repo so it’s located at: {REPO}/.github/workflows/goodreads-books-workflow.yml.\nGitHub Actions look for *.yml files in the /.github/workflows directory. These files are called workflows.\nIn this workflow file you can paste the following:\nname: Latest book list from goodreads on: schedule: # Run workflow automatically # This will make it run every hour - cron: \"0 * * * *\" # Run workflow manually (without waiting for the cron to be called), through the Github Actions Workflow page directly workflow_dispatch: jobs: update-readme-with-goodreads-books: name: Update this repo's README with what you're currently reading runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: zwacky/goodreads-profile-workflow@main with: # Replace this with your goodreads user id (go to \"My Books\" on goodreads to see it in the URL) goodreads_user_id: \"92930971\" shelf: \"currently-reading\" The last lines are input parameters. Make sure you change them with your Goodreads ID.\nHave a look at all the input parameters and template variables that goodreads-profile-workflow support.\n#3 Commit and trigger the workflow If you head over to the Actions tab in the repo. You should see your “Latest book list from goodreads” workflow. This is also where you can see if the job ran successfully or if there is an issue or typo.\n  Alternatively you could also wait until the scheduler picks up on the \"0 * * * *\" setting (every hour) in the yml workflow.\n#4 Done 🎉 Hope you find this workflow useful! Please reach out via GitHub issues or Twitter. Excited to hear from all the different reading lists and how your profile looks. Please consider leaving a GitHub star.\n If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. Hey are you a developer who likes to read?\nWant to automatically show what books you're currently reading in your #GitHub profile page?\nThen check out this fresh workflow for GitHub Actions that will fetch any of your book shelves on #Goodreads.\n👉 https://t.co/CvRwxJpaLT pic.twitter.com/LjNUIq5q5D\n— Simon Wicki (@zwacky) March 31, 2021  ","description":"Automatically sync your Goodreads books you read on your GitHub profile with the help of GitHub Actions.","image":"github-actions-goodreads-cover.png","keywords":["github","actions,","github","workflow,","github","profile,","cron,","goodreads"],"link":"/posts/2021-04-goodreads-workflow-for-github-actions/","title":"Pimp Your GitHub Profile with Books You Read"},{"content":"Npm comes with a neat tool called npx that lets you directly execute packages from the npm registry. It temporarily downloads it behind the scene and won’t pollute your local or global npm environment.\nIn this post I’ll skip useful packages that are bound to libraries and frameworks like Angular’s ng, react’s create-react-app or capacitor’s cap.\nI focus on packages that have helped me through my daily life as a developer.\nnpx npkill   usage: npx npkill\nrepo: npkill\nIt’s no secret that node_modules folders are the black holes of software development. This package will help you get back your disk space by selecting and deleting the node_modules of your (unused) projects.\nnpx kill-port   usage: npx kill-port 8080\nrepo: npkill\nDid some webpack dev server not shut down nicely and now 8080, or any port for that matter, is in use? Just kill it with kill-port.\nnpx http-server   usage: npx http-server .\nrepo: http-server\nEasily serve a directory as a simple webserver. Especially useful for compiled webapps in /build folders.\nnpx timezone-compare   usage: npx timezone-compare\nrepo: timezone-compare\nThis package helped me to plan quickly setting up meetings with others of different timezones.\n Honorable Mention: npx emoj   usage: npx emoj happy\nrepo: emoj\nI used this package to death! Sadly its API, that came from getdango’s Emoji \u0026 Deep Learning, is not maintained anymore. Thus the emoji results are not ground breaking anymore.\nAs a replacement I suggest using ctrl + cmd + space (macOS), that will help you a bit on the way of finding the right emoji.\n If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. 1/ Let me list a few useful npm packages that I never install but still use in my daily life as a frontend developer 👇🧵\n— Simon Wicki (@zwacky) February 18, 2021  ","description":"Check out 5 useful npx packages for killing ports, deleting node_modules folders and comparing time zones.","image":"npm.png","keywords":["npx,","npm,","useful","npm","packages,","developer"],"link":"/posts/2021-02-useful-npx-packages/","title":"Useful Npx Packages for the Developer's Everyday Life"},{"content":"  I’ve used Google Fonts in prototypes and in 10M+ MAU products. It’s incredibly easy to get started with and provides an amazing font discovery. That’s also why it’s currently still used on over 42M websites!\nThis convenience has its price: Performance. Many have already pointed out the cost of multiple requests. If you want the remaining speed boost, then you’re best off downloading your used Google Fonts and self-host them.\nThis is nothing new. In fact it’s been advocated already for years. Even Google themselves advised others to self-host fonts in their Google I/O ‘18 talk about web performance.\nSelf-hosting fonts vs Google Fonts By nature Google Fonts, even with all its font and CSS optimisations, can’t be faster than self-hosted fonts.\nSia wrote a great post where she compared the performance between Google Fonts and self-hosted fonts without the impact of a CDN.\n  Optimised Google Fonts loading with preconnect\n    Optimised self-hosting fonts with preload\n   The old performance argument So if the bottom-line performance is in self-hosting fonts’ favour: What was the argument that convinced us developers that Google Fonts is at least as performing as the self-host approach?\nGoogle Fonts was designed to be distributed on a global CDN and reap the caching benefits from it. Users request fonts via said CDN. Chances are that they have downloaded the font resources at an earlier point already from a different site.\n “[…] Our cross-site caching is designed so that you only need to load a font once, with any website, and we’ll use that same cached font on any other website that uses Google Fonts.”\n— https://fonts.google.com/about\n Invalidating the old performance argument Since Chrome v86, released October 2020, cross-site resources like fonts can’t be shared on the same CDN anymore. This is due to the partitioned browser cache (Safari has had this for years already).\n In this Google post they explain what the partitioned browser cache is. It got only introduced to prevent a possible cross-site tracking mechanism.\nCache partitioning in other browsers Safari really cares about privacy. It circumvented this very cross-site tracking attack for years already. Then finally comes Chrome. Other browsers that are based off Chromium, still need to signal or implement the feature.\n ✅ Chrome: since v86 (October 2020) ✅ Safari: since 2013 🚫 Firefox: planning to implement 🚫 Edge: most likely soon 🚫 Opera: most likely soon 🚫 Brave: most likely soon 🚫 Vivaldi: most likely soon  Conclusion Google Fonts resources will be redownloaded for every website, regardless it being cached on the CDN. Self-host your fonts for better performance. The old performance argument is not valid anymore.\nThanks for checking this post out!\n If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. I wrote a friendly reminder that your web apps are better performing without Google Fonts 🚀👇#frontend #javascript https://t.co/8OEuNWIHyy\n— Simon Wicki (@zwacky) December 1, 2020  ","description":"This browser caching change kills the utility of cross-site resource CDNs like Google Fonts.","image":"goodbye-google-fonts.jpg","keywords":["Google","Fonts,","CDN,","browser","partitioned","cache"],"link":"/posts/2020-11-goodbye-google-fonts/","title":"Time to Say Goodbye to Google Fonts"},{"content":"I love reading books—I read through 1-4 books a month. That wasn’t always the case. If you saw my home just two years ago you wouldn’t have found any books. Back then I read a few blog posts online and that was it. I never felt the need to read pieces of paper bundled together.\n…Until I tried it. I first tried it with a book called Factfulness by Hans Rosling. Backed with studies, science and nice graphs it gives you a great insight in the developed and developing world. That was ideal for me to get my feet wet. And I liked it a lot! It conciously and subconsciously changed my way of looking at things.\nMaybe one or two of my recommendations will pique your interest. Make sure to holler at me on Twitter what your favourites are; I love hearing about your own book takeaways!\nMindset   This book easily left the biggest impact on me. Let me ask you this question: Do you think a professional pianists play well because they are just born with talent and stuck with it? Or do you think it’s because they invest hours of effort in trying to become better and improve themselves?\nIf you answered yes to the first question, it derives from having a fixed mindset, where if you answered yes to the second, it is reflective of a growth mindset. Dr. Carol Dweck carefully explains these two mindsets in her book after 25 years of research in the field of psychology. You’ll understand the advantages of having a growth mindset and what impact it could have on you—and even on your children.\nIf you plan only to read a single book this year, I would recommend this one.\nBad Blood   This book reads itself like a Hollywood Deception Drama—except that it is real! Prepare yourself for an amazing investigative work by John Carreyrou telling you the story of how a Silicon Valley Startup got to the point of an $9 billion evaluation. The only catch? Their product did not work at all.\nYou may have heard of Theranos, the company led by turtle-neck-lover Elizabeth Holmes. They promised that their device could immediately do an array of health diagnostics with only a drop of blood taken from the fingertip. In this page turner you will see how far the executives went with their lies and lawsuits for their cover up.\nNonviolent Communication   This book opened my eyes to something I always knew existed, but I had never explicitly reflected on at length: Empathy. Marshall B. Rosenberg was a psychologist, worked world wide as a peacemaker and started developing Nonviolent Communication in his thirties already.\nThere is something satisfying behind the way the author explains how he gets called in to resolve different issues using the nonviolent communication approach. In one case described the conflict between a staff and their principal of a school. Another case describes what a married couple goes through and how they cleared it up. First the examples look hard to solve but then he unties all the knots.\nI must admit that I’m not 100% on board with how he approaches all and every communication issues. Some steps he’s written about feel a bit hippie-y to me. But nonetheless the quintessence of this book is very valuable.\nAtomic Habits   There exists a plethora of self-help books about productivity and habits, and more are still being written. James Clear is one of the few productivity authors that made it big with this great framework of building good habits and breaking bad ones. It’s even so well known that it’s also the one book my friends and I have most often both read.\nThe two key takeaways are making small changes that will have a big impact over time and packing habits into routines. Imagine you practice a new skill like drawing every day when you wake up and had your coffee (routine) for only five minutes (small changes). Then compare your doodles of when you started with the drawings of one month later. Even if you became only 1% better at it every single day, that 1% is going to add up majorly.\nWhy We Sleep   This book figuratively gave me a wake-up call. I used to sleep only as much as I thought I could get through the day without my body showing signals of sleep deprivation.\nMatthew Walker, the director of UC Berkley’s Center for Human Sleep Science, writes about the results of his studies he conducted of participants with eight, six or even less hours of sleep daily. The conclusion is that we pay the price in many areas if we neglect sleep: Creativity, problem solving, decision-making, learning, memory, heart health, brain health, mental health, emotional well-being, immune system, and even your life span.\nFinal words I hope you enjoyed my short list of recommendations of books that I read in 2019. Have you read a book in common? Do you plan on picking one up? Or do you think there is a great follow up book to the ones mentioned here? I’m super interested to know what you think. Don’t hesitate to tweet about your experience and mention me @zwacky!\n If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. If i can choose only one, it got to be Mindset by Carol Dweck. It helped me change my mindset.\nThis and 4 others big book recommendations I wrote down in a small post https://t.co/hagRGrSJza\n— Simon Wicki (@zwacky) April 11, 2021  ","description":"Read on for my top 5 nonfiction books I finished back in 2019.","image":"cover-mindset.png","keywords":["books"],"link":"/posts/2020-09-my-top-5-book-recommendations/","title":"My Top 5 Book Recommendations"},{"content":"  I launched Notyfy on Product Hunt on 7th of May.\n  My first Product Hunt Launch 🎉—a lot of time went into the preparation together with my friend Peter 👋. Read on to see what stats and learnings finishing #11 on Product Hunt brings.\nNotyfy Let me quickly run you through what the product is that I launched on Product Hunt:\nNotyfy is a browser extension that aggregates web notifications from Twitter, Reddit, Facebook and many more in one place. It is able to see which of the supported platforms you are already logged in — without requiring sensitive credentials like passwords—and does the checking for new notifications for you. Super seamless. Give it a try!\nStats   Notyfy’s install stats on Chrome Web Store platform\n   +218 Product Hunt upvotes +165 New Users (+151%) #11 Product of the Day 19.9% Install Conversion  Let’s quickly talk about the Install Conversion: It’s a calculation with the data I had available (I’m not using any tracking inside the extensions themselves). I was surprised that having something so frictionless and without sign ups could yield such a high conversion: 19,9%!\nI used the following formula:\n Conversion = (Chrome Installs + Firefox Installs) / Landing Page Views\n19,9% = (73 + 13) / 432\n Learnings  📢 Hunter’s followers aren’t notified: After some testing we saw that none of the hunter’s followers were notified upon the new product launch. On the other hand the Maker’s followers were notified. Therefore best add the Hunter as the Maker as well, to get that initial upvote traffic from the Hunter’s followers as well. 👀 Catch the user’s attention: It’s very crucial to have a catchy logo and an understandable tagline 📅 Set a launch date: The preparation took a lot of time and the longer it took the less results came out of it. It took even so long that once a random user hunted Notyfy by themselves! No prepared texts, assets and logos were present at that time and the random launch didn’t exceed 10 upvotes. A friendly mail to Product Hunt solved this issue within 1 hour 🎉 🗣 Interact with comments: Engage with every comment on the launch page by replying to comments upvoting other comments. It’s highly probable that this impacts the front page ranking algorithm, too. 🙅‍♀️ Don’t fear haters: Launching on Product Hunt exposes what you’ve worked on and that can be scary. But these comments usually are well structured and should be taken as feedback to make an even better product!  ","description":"Stats, stats, stats!","image":"notyfy-product-hunt-badge.png","keywords":["product,","chrome","extension"],"link":"/posts/2020-06-stats-and-learnings-from-finishing-11-on-product-hunt/","title":"Stats and Learnings from Finishing #11 on Product Hunt"},{"content":"…and other things I’ve heard regarding Hybrid App development.\nLet’s talk about the elephant in the room: Performance. Let me just show you how fast and smooth Hybrid apps can be based on this footage of my last work: JustWatch, a streaming search engine with 12M monthly active users. It’s been recorded on a OnePlus 6 (May 2018).\n  “Hybrid Apps will never be faster than native” Yes, this is true. But they also don’t need to be. With current hardware and browser engines the performance of Hybrid Apps is already way beyond what Facebook’s Hybrid App debacle delivered in 2012. With a good app structure, avoiding common performance pitfalls and applying good UX, your app can achieve a great performance.\n“Hybrid Apps get rejected by Apple” Every now and then you’ll see a stream of Hybrid App developers tweeting about their app’s rejection by Apple. The causes are usually the same: bad usability and design, not Apple systematically sorting out Hybrid Apps.\nFake emails have been circulating stating apple would shut down apps built with Xamarin, PhoneGap and Appcelerator: These are a hoax.\nPosts are getting more attention saying Apple wants to kill web technologies: These are unfounded opinions of other developers causing FUD.\n“You can’t do that with a Hybrid App” Yes, there are limitations where Hybrid Apps are not the right tool for the job. I’m just going to stick my neck out there and say that most app requirements will never meet these limitations.\nThe native capabilities are covered in Hybrid Apps with Cordova plugins—or better and more painless with Capacitor plugins. Even though some of the Cordova ones are a pain to implement.\n“There are no big Hybrid Apps in the app stores” Have a quick look at Ionic’s Showcase site: All these apps boast at least 0.5M downloads on a single platform.\nBesides the JustWatch app—which currently has 3M downloads for Android and 4M for iOS—there are some significantly larger apps built on Ionic. Some can be shared and some that don’t want tech details talked about publicly.\n“Hybrid Apps don’t look native” You may have come across your fair share of apps that are basically HTML packed into a WebView: Narrow UI elements to tap on, feedbackless interactions, terrible transitions and more atrocities. I have, too.\nFrameworks like Ionic have put a lot of effort in the past years into the look and feel and make their components usable and extendible with: Immediate feedback upon interaction, defaulting to the right size for your finger, seamless and jitter-less lazy loading, following platform guidelines and much more.\nWrapping up: Hybrid Apps vs Native Apps In the past people only knew one reason why Hybrid Apps were built: Money needed to build the app quicker on several platforms. But gradually Hybrid Apps got faster and smoother and trade offs became smaller and smaller. In case of this, other factors were getting more attention:\n Desired iteration speed across one or multiple platforms Your existing team structure, knowledge and hiring And again: Money   If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. \"Hybrid Apps are slow\"—I've heard many times. So I recorded this video footage of the recent Ionic 4 app rewrite I've done! 🔥🎉 #capacitor #ionic #vue #typescript\nCheck out more Hybrid App myths here: https://t.co/JXdGLfaiQm pic.twitter.com/oaapmMgsuI\n— Simon Wicki (@zwacky) November 20, 2019  ","description":"…and other things I’ve heard regarding Hybrid App development.","image":"hybrid-apps-are-slow.jpg","keywords":["hybrid","apps,","capacitor,","ionic"],"link":"/posts/2019-11-hybrid-apps-are-slow/","title":"\"Hybrid Apps are slow\""},{"content":"  MARCH 2020 UPDATE: The new Firebase JS SDK Alpha makes your bundle up to 80% smaller! It now fully adopts tree-shaking and lets you import only what you truly need.\n In a lot of tutorials I came across the comfortable solution of just adding import * as firebase from ‘firebase’ for your firebase import. As it turns out, there are 4 modular packages for Firebase. Here they are along with their sizes:\n firebase/app: 22.1kb .initializeApp(firebaseConfig) firebase/auth: 147.6kb .auth() firebase/database: 139.8kb .database() firebase/storage: 51.2kb .storage()  Usually you might not need all of these Firebase services and can require() or import just what is needed — with the help of webpack’s Tree Shaking.\nCode before:\nimport * as firebase from 'firebase'; after:\nimport * as firebase from 'firebase/app'; import 'firebase/storage'; import 'firebase/database'; // ...  firebase.initializeApp(firebaseConfig); This will tell webpack (or whatever bundler you use) that they should also include storage and database. You can then still use firebase.storage().ref(...) in your code.\nApplied example Let’s assume you only want to take advantage of Firebase’s great Database. Instead of adding 360.7kb, you’d only have to add the firebase/database that imports 139.8kb.\nResulting in 220.9kb saved (61%)!\n","description":"Easy steps to follow to ensure you're not importing too much.","image":"firebase-webpack.png","keywords":["firebase,","webpack"],"link":"/posts/2017-08-using-firebase-you-might-be-able-to-save-220kb/","title":"Using Firebase with webpack? You might be able to save 220kb"},{"content":"  Nearly all evergreen browsers support Async/Await natively\n  Async/Await has been around the block already some time. Now that it is in stage-4 since July 2016 (stage finished in the ECMAScript proposals) and nearly all evergreen browsers support it natively, too (except IE is late to the party as usual 😪) — it’s definitely time to take a second look.\nTL;DR I WANT ASYNC/AWAIT SUPPORT NAO!!1  use babel-preset-env yarn add regenerator or npm install regenerator add node_modules/regenerator-runtime/runtime.js (10.7kb minified) into your bundle  #1 Requirements #1.1 Babel Forget preset-stage-0, babel-plugin-syntax-async-functions or whatever you can still find on outdated resources. Just use babel-preset-env.\nYour .babelrc could look like this for the bare minimum to work (add your fancy plugins and more if needed):\n{ \"presets\": [ [ \"env\", { \"targets\": { \"browsers\": [\"last 2 versions\", \"IE \u003e= 9\"] } } ] ] } If you’re like me, then you can’t just forget about Internet Explorer ≥ 9. babel-preset-env lets you specify a browserlist config that calculates your needs and then automatically takes care of all the needed babel plugins.\nNote: The babel plugins do not bloat your bundle file. Polyfills do:\n#1.2 Polyfills  An argument could be made for the transform to inject an import for babel-regenerator-runtime automatically, but generally Babel has shied away from automatically injecting imports since there is no guarantee that the user will be using a module system, so the global route was taken. (from GitHub Issue December 2015)\n Babel has shied away of automagically adding polyfills for generators. Therefore you need to add your own polyfill. There is a babel-polyfill that adds all possible polyfills and has a proud size of 97kb minified. If you only need the generator polyfill — which is needed for async/await — then you can just use facebook/regenerator, which is used by babel-polyfill anyway.\n yarn add regenerator or npm install regenerator –save (no dev dependency because it will be injected into your frontend bundle) add node_modules/regenerator-runtime/runtime.js (10.7kb minified) into your bundle  Or don’t use a polyfill at all.\nIf you have set your browser targeting to chrome \u003e= 39 then you simply will use the native generators and will never make use of the polyfill. But then again, you’ll save 10kb minified here. Might be easier to add the polyfill and later on make the webapp available for IE9.\n#2 Linting (ESLint) If you’re still using ES6 to write JavaScript, then you probably came across ESLint. It also comes with a neat VS Code extension, that will read your .eslintrc.json correctly and lint accordingly.\nYou need to tell your .eslintrc.json config, that you’re now playing with the big tools and tell it to lint for ECMAScript version 8:\n{ \"env\": { \"browser\": true, \"commonjs\": true, \"es6\": true, \"node\": true }, \"parserOptions\": { \"ecmaVersion\": 8 }, \"rules\": {} }  If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. Use this to enable ES7 async/await in your @babeljs build: https://t.co/2PsnQqW4Tc 🙋🚀 #babel #es7 #javascript\n— Simon Wicki (@zwacky) June 26, 2017  ","description":"Step by step guide to add async/await in your build flow.","image":"caniuse-async-await.png","keywords":["es7,","async","await"],"link":"/posts/2017-06-add-es7-async-await-support-into-your-non-bleeding-edge-build-process/","title":"Add ES7 Async/Await Support for your Webapp in 3 Easy Steps"},{"content":"  New Update available popup.\n  PWAs are getting more and more coverage and support. They improve the web experience and can load your app instantly with their great ability for HTTP caching (among other things, but this post only covers caching).\nThe thing with Offline-First is, that you cache all the resources that are needed for launching up the webapp — even your index.html!\n Note: No sweat! Browser implementations prevent you from deploying a version that will be cached for all eternity. For instance Chrome treats max-age of 1 day or 1 week or 1 year as 24 hours.\n #1 Structure For example reasons, the 3 needed files are linked to the source of an actual PWA (gamemusicplayer.io) for a clearer understanding.\n index.html — registers the service-worker.js and is wrapped inside the isUpdateAvailable Promise service-worker.js — defines what files will be cached somewhere inside your webapp — listens to isUpdateAvailable Promise and acts accordingly  #2 Why can’t it just load the latest version? When the browser loads your PWA, it doesn’t know if there is a new version available. It promptly loads the cached assets. There are different cache strategies, depending on your use case.\n networkOnly – only fetch from network cacheOnly – only fetch from cache fastest – fetch from both, and respond with whichever comes first networkFirst – fetch from network, if that fails, fetch from cache cacheFirst – fetch from cache, but also fetch from network and update cache  In 99% of the cases you can decide on 2 user experiences:\n Load the page instantly from the cache Check first if network is available, otherwise load from cache as a fallback  Both options are offline-first. The 2nd option, if network is available, will therefore always deliver the most updated version, but with a delay.\nIf you want to give your users that ⚡️-fast page load experience and notify them when a newer version of their cached version is available, you’d need to hook into the onupdatefound function in your Service Worker.\n#3 How to check if your cached files have changed We can hook into onupdatefound function on the registered Service Worker. Even though you can cache tons of files, the Service Worker only checks the hash of your registered service-worker.js. If that file has only 1 little change in it, it will be treated as a new version.\n#3.1 Register service-worker.js The following code should be inside \u003cscript\u003e tags in your index.html. It will add a isUpdateAvailable function to the global scope, so it can later be used as a Promise.\n// make the whole serviceworker process into a promise so later on we can // listen to it and in case new content is available a toast will be shown window.isUpdateAvailable = new Promise(function (resolve, reject) { // lazy way of disabling service workers while developing  if ( \"serviceWorker\" in navigator \u0026\u0026 [\"localhost\", \"127\"].indexOf(location.hostname) === -1 ) { // register service worker file  navigator.serviceWorker .register(\"service-worker.js\") .then((reg) =\u003e { reg.onupdatefound = () =\u003e { const installingWorker = reg.installing; installingWorker.onstatechange = () =\u003e { switch (installingWorker.state) { case \"installed\": if (navigator.serviceWorker.controller) { // new update available  resolve(true); } else { // no update available  resolve(false); } break; } }; }; }) .catch((err) =\u003e console.error(\"[SW ERROR]\", err)); } }); // Update: // this also can be incorporated right into e.g. your run() function in angular, // to avoid using the global namespace for such a thing. // because the registering of a service worker doesn't need to be executed on the first load of the page. #3.2 Check if update is available In this example i’m using Ionic 3 to easily display a toast that will tell the user that there has been an update — in case of an update.\n#4 Caveats Problems can arise when you use a hosting service, that automatically adds max-age headers to your resources — especially your service-worker.js.\nFor instance if you host your PWA over Firebase Hosting, you’ll find this configuration useful.\n(Bonus: the public folder is set to ./platforms/browser/www/ because Ionic 3 makes it very easy for PWAs from start to finish!)\n{ \"hosting\": { \"public\": \"./platforms/browser/www/\", \"rewrites\": [ { \"source\": \"**\", \"destination\": \"/index.html\" } ], \"headers\": [ { \"source\": \"service-worker.js\", \"headers\": [ { \"key\": \"Cache-Control\", \"value\": \"max-age=0\" } ] } ] } } #5 Summary Service Workers aren’t as scary as they seem at first. With the appropriate safety mechanisms in place (never cache more than 24 hours) you can create a great experience for your users without having to change your domain name.\n If you found this post interesting please leave a ❤️ on this tweet and consider following my 🎢 journey about #webperf, #buildinpublic and #frontend matters on Twitter. \"New Update Available\" using Service Workers with Progressive Web Apps? 📱⚡️ #PWA #ionic #javascript #offlinefirst https://t.co/yl5G2AQFa3\n— Simon Wicki (@zwacky) June 14, 2017  ","description":"Dive into the world of PWAs covering service workers, cache busting and offline-first.","image":"new-update-available-popup.gif","keywords":["PWA,","service","workers,","offline-first,","cache","busting"],"link":"/posts/2017-06-pwa-create-a-new-update-available-notification-using-service-workers/","title":"PWA: Create a \"New Update Available\" Notification using Service Workers"},{"content":"Lately I encountered some native build errors that slowed me down quite a bit. Here is an overview of some of them, their issue and solution which might save you some time if you come across one of the following native errors:\n iOS: ‘GoogleCloudMessaging.h’ file not found iOS: Duplicate Symbols Android: safeparcel.AbstractSafeParcelable not found Android: Force Close due to phonegap-push-plugin    iOS: ‘GoogleCloudMessaging.h’ file not found error: ‘GoogleCloudMessaging.h’ file not found #import \"GoogleCloudMessaging.h\" Lexical or Preprocessor Issue \u003e ‘GoogleCloudMessaging.h’ file not found \u003e PushPlugin.m Issue Cordova supports CocoaPods since 4.3.0. The issue here is that 2 pods don’t load correctly and you need to add them manually in your ./platforms/ios/Podfile like in the solution.\nSolution platform :ios, '7.0' target 'JustWatch' do pod 'GoogleCloudMessaging' pod 'GGLInstanceID' end …and then in ./platforms/ios do a pod install.\n iOS: Duplicate Symbols ld: 270 duplicate symbols for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) ** BUILD FAILED ** Issue: phonegap-plugin-push and cordova-plugin-googleplus have common dependencies, but both add them separately which causes the duplicate symbols.\nSolution:  Open your project with XCode. Go to “Build Phases” and access to “Link Binary With Libraries” Remove these libraries:  - libGTMSessionFetcher_core.a - libGTMSessionFetcher_full.a - libGTM_AddressBook.a - libGTM_core.a - libGTM_GTMURLBuilder.a - libGTM_iPhone.a - libGTM_KVO.a - libGTM_Regex.a - libGTM_StringEncoding.a - libGTM_SystemVersion.a As suggested here by @ideatia: https://github.com/phonegap/phonegap-plugin-push/issues/449#issuecomment-223557733\n Android: safeparcel.AbstractSafeParcelable not found PluginUtil.java:135: error: cannot access AbstractSafeParcelable Builder builder = LatLngBounds.builder(); ^ class file for com.google.android.gms.common.internal.safeparcel.AbstractSafeParcelable not found Issue: Two plugins are using the same play services dependencies but with different versions that are explicitly set and therefore can’t use two different dependency versions.\nSolution: Unpin the play-services-map and play-services-location by adding this Cordova plugin fork directly: ionic plugin add https://github.com/zwacky/cordova-plugin-googlemaps\n Android: Force Close due to phonegap-push-plugin E/AndroidRuntime( 2059): FATAL EXCEPTION: pool-1-thread-3 E/AndroidRuntime( 2059): Process: com.justwatch.justwatch, PID: 2059 E/AndroidRuntime( 2059): java.lang.NoSuchMethodError: No static method getNoBackupFilesDir(Landroid/content/Context;)Ljava/io/File; in class Lcom/google/android/gms/common/util/zzw; or its super classes (declaration of ‘com.google.android.gms.common.util.zzw’ appears in /data/app/com.justwatch.justwatch-1/base.apk) E/AndroidRuntime( 2059): at com.google.android.gms.iid.zzd.zzkq(Unknown Source) E/AndroidRuntime( 2059): at com.google.android.gms.iid.zzd.\u003cinit\u003e(Unknown Source) E/AndroidRuntime( 2059): at com.google.android.gms.iid.zzd.\u003cinit\u003e(Unknown Source) E/AndroidRuntime( 2059): at com.google.android.gms.iid.InstanceID.zza(Unknown Source) E/AndroidRuntime( 2059): at com.google.android.gms.iid.InstanceID.getInstance(Unknown Source) E/AndroidRuntime( 2059): at com.adobe.phonegap.push.PushPlugin$1.run(PushPlugin.java:74) E/AndroidRuntime( 2059): at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1112) E/AndroidRuntime( 2059): at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:587) E/AndroidRuntime( 2059): at java.lang.Thread.run(Thread.java:818) Issue: Google Play GCM dependencies have changed here which made the phonegap-push-plugin have to adjust accordingly, otherwise wrong method signatures will be called which causes a force close.\nSolution: Update to latest phonegap-push-plugin version or basically everything after 1.8.4.\n","description":"A short compilation of some iOS \u0026 Android native build errors.","image":"native-build-error.png","keywords":["cordova"],"link":"/posts/2017-03-ios-android-native-build-errors-with-cordova-plugins/","title":"iOS \u0026 Android Native Build Errors with Cordova Plugins"},{"content":"Demo https://jsbin.com/gutahovufe/1/edit?html,js,output\nExplanation Sometimes you need to execute code after your list has been rendered on the client side.\n// js angular.module('ngRepeatDemo', []) .controller('AppCtrl', function() { var vm = this; vm.alphabet = [ 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z' ]; vm.finished = function() { alert('fired'); // now javascript execution has been stopped and you should see all DOM nodes been created  // note: interpolation may not have been done completely  }; }); \u003c!-- html --\u003e \u003cdiv ng-repeat=\"letter in app.alphabet\" ng-init=\"$last \u0026\u0026 app.finished()\"\u003e {{letter}} \u003c/div\u003e The whole magic comes from the ng-init part. the $last makes sure, that it only gets fired, when the last element has been rendered to the DOM.\nThis will also fire whenever the list changes and a change has been issued by the digest cycle.\n","description":"Useful trick to make sure ng-repeat has finished rendering.","image":null,"keywords":["angularjs"],"link":"/posts/2015-09-angular-s-ng-repeat-finish-event/","title":"Angular’s ng-repeat Finish Event"},{"content":"For external redirection\nThe use case here is to redirect the user to another domain. I haven’t come across Redirect::away() before, even though it’s been added in Laravel 4.0.8.\nRedirect::to()   Redirect::away()   Difference Redirect::to() does additional URL checks and generations. Those additional steps are done in Illuminate\\Routing\\UrlGenerator and do the following, if the passed URL is not a fully valid URL (even with protocol):\n Determines if URL is secure rawurlencode() the URL trim() URL  ","description":"The difference between Laravel's URL::to and URL::asset.","image":null,"keywords":["laravel"],"link":"/posts/2014-09-laravel-redirect-to-vs-redirect-away/","title":"Laravel Redirect::to() vs Redirect::away()"},{"content":"  $_ — last output maybe you don’t want to store results in a variable for quick calculations and just want to chain those computations.\n  $0 — currently inspected element useful if you want to output or alter the selected element.\n  angular.element($0).scope() — get AngularJS’s scope combined with $(0) this makes a great use to inspect a specific element and then check what it’s scope looks like.\n  console.time(str) and console.timeEnd(str) — calculate the passed time who hasn’t fiddled around with new Date(); or similar to profile a certain action in the code.\nJust pass an identifyable string to each function call, so that the dev tools know where to start and to end.\n  The same is possible for CPU profiling using console.profile(msg) and console.profileEnd(msg).\n","description":"Check out these useful Chrome Dev Tool commands.","image":"chrome.jpeg","keywords":["hybrid","apps,","capacitor,","ionic"],"link":"/posts/2014-07-very-useful-chrome-dev-tool-commands/","title":"4 Very Useful Chrome Dev Tool Commands"},{"content":"Short answer: No, not really.\nYou want to have the following code:\n\u003cscript src=\"js/bootstrap.js\"\u003e\u003c/script\u003e Now you can achieve this using blade syntax:\n// using URL::to() \u003cscript src=\"{{ URL::to('js/bootstrap.js') }}\"\u003e\u003c/script\u003e // using URL::asset() \u003cscript src=\"{{ URL::asset('js/bootstrap.js') }}\"\u003e\u003c/script\u003e URL::to() The method is implemented like this:\n  URL::asset() The method is implemented like this:\n  Difference Both methods get the job done.\n URL::to() additionally encodes the segments of the passed url with rawurlencode URL::asset() removes index.php from the path (which shouldn’t be in first place)  So you get more utility by using URL::to(), but also takes longer to execute (approx. 25% more than URL::asset()). But it’s strongly neglectible, since calling 100 times URL::to() takes 0.003523 ms.\nAlternatives If you’re using blade, you can use the following to easily output style and script tags.\n{{ HTML::script('js/bootstrap.js') }} {{ HTML::style('css/bootstrap.min.css') }} Or using any of Asset Management packages like orchestra/asset.\n","description":"The difference between Laravel's URL::to and URL::asset.","image":null,"keywords":["Laravel"],"link":"/posts/2014-04-laravels-url-to-vs-url-asset/","title":"Laravel's URL::to() vs URL::asset()"},{"content":"Every dev at some point has made decisions how his backend API should look like. If you’re coming from the web, then mostly for ajax calls. But what if that API should also address calls from other sources and clients.\nTo fill that gap i was watching an informative talk by Les Hazlewood about beautiful RESTful. I’ll cut this in several sections. This is merely an overview, the full coverage you can get from the youtube talk or the slides.\nEndpoint Naming  Nouns, not verbs (No: getUser, Yes: /users/a1b2c3) Parameters not in URL (hassle if changes arise api.com/do/param1/param2/param3)  It should be divided into two fragments:\n Collection (api.com/users) Instance (api.com/users/a1b2c3)  Behaviour It’s a good thing to use different methods for calling the api. They don’t represent 1:1 the CRUD model. PUT and GET can be used for create and update.\n GET: read PUT: create, update POST: create, update (partial update, due to quota) DELETE: delete head: headers, no body  Update / Create Keyword here is idempotent. This means that it produces the same result if it’s been sent once or multiple times. To avoid having different states, the update should contain the full object in its entirety.\nPartial updates can be done (using POST) if you’re concerned about your data quota.\nYou should give responses back for the PUT/POST:\n 201 OK (or 201 created for a create) location, where instance can be found (url: http://api.com/users/g4h5i6)  Media Types They tell the the server what format the client understands and prefers and how the client has to parse and process it.\n Request: accept header Response: content-type header  you can have your own media-type (application/json+foo), that has a defined specification or add additional parameters (application/json+foo;application) — see API Versioning for another possible use case.\nAPI Versioning In case you make drastic changes, you still want to support the old API, to prevent problems with old implementations. There are possibilities to do so:\n URL: api.com/v1 media-type: application/json+foo;application,v=1  Linking You want to retreive a resource, which contains other resources. Best practice is to provide an atomic resources. So you shouldn’t provide an endpoint as such asapi.com/UserWithAddress, rather have seperate endpoints for each. In a JSON it would look as follows:\n{ \"href\": \"http://api.com/users/a1b2c3\", \"name\": \"John\", \"lastname\": \"Smith\", \"address\": { \"href\": \"http://api.com/addresses/d4f5g6\" } } The result doesn’t contain the all address properties, but the href one, which tells the requester exactly where the resource is located.\nReference Expansion In case you’d like to make only one request for the user and its address properties, you can provide an additional parameter:\n api.com/users/a1b2c3?expand=address  You can also request more than one reference expansion separated by a comma. In case the property doesn’t exist, a good practice is to return a 409 Conflict.\nPartial Representation Usually you get all properties of a resource upon a request. You could list the wanted properties in a request like this to save bandwith:\n api.com/users/a1b2c3?expand=address(street,zip)  Pagination On Collection calls you may come across a lot of items that shouldn’t be requested all in one call. A possible pagination request could look like this:\n api.com/users?offset=0\u0026limit=25  The returning JSON could look like the following:\n{ \"offset\": 0, \"limit\": 25, \"first\": { \"href\": \"http://api.com/users?offset=0\" }, \"previous\": null, \"next\": { \"href\": \"http://api.com/users?offset=25\" }, \"last\": { \"href\": \"http://api.com/users?offset=425\" }, \"items\": [{}, {}] } Errors Sometimes errors should tell the API caller more than only a return code, then this is a good way to do so.\n as descriptive as possible as much information as possible if your API is targeted at other devs, provide additional dev info  POST /users → 409 conflict:\n{ \"status\": 409, \"code\": 40924, \"property\": \"name\", \"message\": \"user named xyz already exists\", \"developerMessage\": \"user named xyz already exists. if you have a stale local cache, please expire it now\", \"moreInfo\": \"http://…\" } IDs  should be opaque should be globally unique avoid sequential numbers good candidates: uuids, url64  What the talk covers additionally In the 1h 25min talk, Les Hazlewood covers other topics. Hop over, if you’d like to know more about these sections:\n Base URL Many to Many Return Values Method Overloading Caching \u0026 Etags Security Multi Tenancy Maintenance  ","description":"Learn how to design a beautiful REST API.","image":null,"keywords":["api"],"link":"/posts/2013-12-design-a-beautiful-rest-api/","title":"Design a beautiful REST API"},{"content":"","description":"Check for Google Fonts on a website and download to easily self-host them.","image":null,"keywords":null,"link":"/google-fonts-converter/","title":"Convert Google Fonts to Self-Hosted fonts"},{"content":" Angaben gemäß § 5 TMG  Simon Wicki\nFreiberuflicher Softwareentwickler\nAlt-Moabit 77\n10555 Berlin\nGermany\n  USt-IdNr  DE326971393\n  Kontakt  simon@wicki.io\n  Verantwortlich für den Inhalt nach § 55 Abs. 2 RStV  Simon Wicki\n  Haftungsausschluss  Haftung für Inhalte\nDie Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.\nHaftung für Links\nUnser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf mögliche Rechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.\nUrheberrecht\nDie durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.\n  ","description":"Imprint of wicki.io.","image":null,"keywords":null,"link":"/imprint/","title":"Imprint"},{"content":" TL;DR  wicki.io does not store any data.\n  Google Analytics  This site uses Google Analytics (GA) for statistical purposes only. GA is great for analytics but it’s also part of a bigger construct of sharing and using user data. Site operators are able to configure and turn off these features. Here is an overview of the steps that have been taken to ensure the privacy of this site’s visitors.\n IP Anonymization: ✅ Disable advertising features: ✅ Disable advertising personalization: ✅ Disable sharing data with Google products \u0026 services: ✅ Disable benchmarking of anonymous data: ✅ Disable access for Google technical support: ✅ Disable access for Google marketing specialists: ✅ Disable access for Google sales experts: ✅ Retention period for the Analytics data: 14 months (shortest)    Google Web Fonts  For uniform representation of fonts, this site uses web fonts provided by Google. When you open a page, your browser loads the required web fonts into your browser cache to display texts and fonts correctly.\nFor this purpose your browser has to establish a direct connection to Google servers. Google thus becomes aware that our web page was accessed via your IP address. The use of Google Web fonts is done in the interest of a uniform and attractive presentation of our website. This constitutes a justified interest pursuant to Art. 6 (1) (f) DSGVO.\nIf your browser does not support web fonts, a standard font is used by your computer.\nFurther information about handling user data, can be found at https://developers.google.com/fonts/faq and in Google’s privacy policy at https://policies.google.com/privacy/.\n  GitHub Pages  GitHub may collect Technical Information from visitors to your GitHub Pages website, including logs of visitor IP addresses, to maintain the security and integrity of the website and service. GitHub Privacy Statement.\n  ","description":"All your data aren't belong to us.","image":null,"keywords":null,"link":"/privacy/","title":"Privacy Policy"}]